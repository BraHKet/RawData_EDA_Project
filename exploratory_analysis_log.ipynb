{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2776c57-4ebb-4303-a553-9a439b4a9a3b",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning and Feature Engineering\n",
    "\n",
    "This section covers the initial steps of data cleaning and feature engineering.  \n",
    "The primary goal is to understand the raw data, handle preliminary issues, and create new, more useful features from the existing ones.\n",
    "\n",
    "## 1.1 Initial Data Inspection and Handling\n",
    "\n",
    "The first step consisted of a preliminary inspection of the raw dataset.  \n",
    "To better understand the content and structure of each column, the DataFrame was exported to an Excel file for quick visual inspection.\n",
    "\n",
    "However, this led to an immediate technical issue.\n",
    "\n",
    "### Problem: Excel Formula Injection\n",
    "\n",
    "Some text fields begin with special characters such as `=`, `+`, `-`, or `@`.\n",
    "\n",
    "When opened in Excel, these are interpreted as formulas, which may corrupt the data or generate errors.\n",
    "\n",
    "To prevent this issue, a helper function was implemented to prepend a single quote (`'`) to any string starting with these characters, forcing Excel to interpret them as plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "061910e4-19e9-4387-af2d-3d7fec0c88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = 'data/apple_jobs.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05524df6-c7f3-4cf6-9bff-a7b40508f7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File caricato con successo!\n"
     ]
    }
   ],
   "source": [
    "# 3. Leggere il file CSV e caricarlo in un DataFrame Pandas\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"File caricato con successo!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Errore: il file '{file_path}' non è stato trovato.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38cb8a7f-b470-4680-b589-2bc33bf0e816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Excel creato con successo: data/apple_jobs_EXCEL_visivo.xlsx\n"
     ]
    }
   ],
   "source": [
    "def escape_excel_formula(x):\n",
    "    if isinstance(x, str) and x.startswith(('=', '+', '-', '@')):\n",
    "        return \"'\" + x\n",
    "    return x\n",
    "\n",
    "df_excel = df.map(escape_excel_formula)\n",
    "\n",
    "# Esportiamo in Excel\n",
    "output_path = 'data/apple_jobs_EXCEL_visivo.xlsx'\n",
    "df_excel.to_excel(output_path, index=False)  # index=False evita di esportare l'indice come colonna\n",
    "\n",
    "print(f\"File Excel creato con successo: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b83676c-e155-4a8d-9738-c8844753b700",
   "metadata": {},
   "source": [
    "## 1.2 Standardizing Job Titles into Role Categories\n",
    "\n",
    "The `title` column is a free-text field containing multiple variations of job roles.\n",
    "\n",
    "The objective is to create a new standardized column called `Role_Category` to group the most common roles into:\n",
    "\n",
    "- **SD** → Software Developer  \n",
    "- **SE** → Software Engineer  \n",
    "- **OTHER** → All remaining roles\n",
    "\n",
    "### Attempt 1: Strict Matching\n",
    "\n",
    "The first approach used regular expressions to match full words such as:\n",
    "\n",
    "- developer\n",
    "- engineer\n",
    "\n",
    "Word boundaries (`\\b`) were used to ensure exact matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "334302c8-ae58-4a0b-850d-4d97368a07b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Role_Category\n",
       "SE       1380\n",
       "OTHER     672\n",
       "SD         96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create lowercase version for matching\n",
    "df['Title_clean'] = df['title'].str.lower()\n",
    "\n",
    "cond_sd_v1 = df['Title_clean'].str.contains(r'\\bdev(?:eloper)?\\b', na=False)\n",
    "cond_se_v1 = df['Title_clean'].str.contains(r'\\beng(?:ineer)?\\b', na=False)\n",
    "\n",
    "df['Role_Category'] = np.select(\n",
    "    [cond_sd_v1, cond_se_v1],\n",
    "    ['SD', 'SE'],\n",
    "    default='OTHER'\n",
    ")\n",
    "\n",
    "df['Role_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d6060-f4bb-4565-a1ee-78dc85ea04e5",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "This approach left a high number of roles classified as **OTHER**.\n",
    "\n",
    "After inspecting these titles, it became clear that many contained abbreviations such as \"eng\" or partial words like \"dev\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9360962-5cc9-499c-9a96-6a2a10ffd0ad",
   "metadata": {},
   "source": [
    "### Attempt 2: Flexible Matching\n",
    "\n",
    "To improve the classification, partial string matching was used instead:\n",
    "\n",
    "- \"dev\"\n",
    "- \"eng\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e19e4657-c2a3-4fde-977c-51cb3b5605a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Role_Category\n",
       "SE       1454\n",
       "OTHER     488\n",
       "SD        206\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_sd_v2 = df['Title_clean'].str.contains(r'dev', na=False)\n",
    "cond_se_v2 = df['Title_clean'].str.contains(r'eng', na=False)\n",
    "\n",
    "df['Role_Category'] = np.select(\n",
    "    [cond_sd_v2, cond_se_v2],\n",
    "    ['SD', 'SE'],\n",
    "    default='OTHER'\n",
    ")\n",
    "\n",
    "df['Role_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b79b93-3965-44d8-a173-b09724c9b380",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "The second approach did not substantially reduce the number of unclassified roles.\n",
    "\n",
    "However, this raises important analytical questions:\n",
    "\n",
    "1. Is the SD vs SE distinction truly meaningful?\n",
    "2. Does the dataset reveal structural differences between these roles?\n",
    "3. Will this feature uncover meaningful patterns, or is it an oversimplification?\n",
    "\n",
    "I'm starting to think that my analysis is too generic. Then, what does it really mean to be a developer rather than an engineer? What weight could this distinction carry in our analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ecc1c-9e2b-47b3-b18b-f6451ef3870c",
   "metadata": {},
   "source": [
    "## Pure Study of the Variables\n",
    "\n",
    "Let's pause and take a breath… we are mixing the two methods. In the first analysis, we should **not modify any variable**, but rather understand the meaning of each one before considering the client’s requirements.  \n",
    "\n",
    "So, if I were to start over, I would use Python code to **visualize the features appropriately and in a readable way**, generating the file **apple_jobs_EXCEL_visivo**. From here, there are two ways to continue:\n",
    "\n",
    "1. Extract every value of the variable (in this case textual) and have the AI explain in detail what it means. This is because it is difficult to manually analyze almost 2,000 unique texts.  \n",
    "2. Get a sense of what the variable is trying to represent by sampling only some of the texts.  \n",
    "\n",
    "I will proceed using the second method first, and then the first one, and compare which approach is more useful by evaluating both speed and efficiency.  \n",
    "\n",
    "### Manual Description of Variables\n",
    "\n",
    "Below is a description of the variables:\n",
    "\n",
    "1. **title:** It is noticeable that the titles start with standard definitions like “Software Engineer,” and then move to more detailed and complex definitions. They are all engineers, but they perform different tasks and have different levels of experience. One could consider subdividing these engineers into more homogeneous macro-categories. For example: front-end software engineers, back-end SE, senior iOS SE, Java SE, etc. To do this without AI, we would need to find keywords for each segmentation (for example: java, iOS, back, front, etc.) and automate the classification by assigning a standard name.  \n",
    "\n",
    "2. **location:** First, we check how many unique values exist. If there are different cities, the problem is standardizing the nomenclature. We can automate this with a Python function that, each time it encounters a new field, compares it with the others stored in memory (as a vector of words). If it mostly matches one of them (for example, only one or two words differ), it assigns the standard name of that location; otherwise, it stores it in memory as a new value.  \n",
    "\n",
    "3. **minimum_qual, preferred_qual, responsibilities, education & experience:** These variables are really difficult to standardize into data without using AI. They describe the minimum and preferred requirements for the job, the responsibilities of the employee, and the education and experience required.  \n",
    "\n",
    "## Objectives of the Work\n",
    "\n",
    "So, we understand that the dataset concerns job openings in certain specific cities. Now the questions we ask ourselves are:\n",
    "\n",
    "1. What information can we extract from the data?  \n",
    "2. Who could benefit from our investigation?  \n",
    "3. What does the client commissioning the study want to know?  \n",
    "\n",
    "From the data, we could understand how jobs are distributed geographically, and how the location affects the difficulty and access requirements for a job as a software engineer. For example, this investigation could serve someone looking for a job who doesn’t know where to relocate. Or it could help other companies looking for employees in that area to understand the competition and align themselves. Variables such as proximity to important universities or work centers could also be included. At the moment, no other practical uses for this dataset come to my mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9953fc9-f447-46bb-b3fd-5128f8936684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
